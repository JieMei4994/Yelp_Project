[["index.html", "Yelp Analysis Chapter 1 Introduction", " Yelp Analysis Jie Mei, Liuxin Chen, Ningxin Li, Yihui Hu 2020-12-18 Chapter 1 Introduction Yelp is an American public company headquartered in San Francisco, California. It is also the most famous mobile app, which contains basic information, ratings, pictures, order service, comments, etc. for shops mostly in US, Canada and Mexico. In 2020, due to the epidemic, the opening and closing times, delivery services, and availability of reservation for many restaurants have all changed. We hope to build a platform to facilitate users to have an overall understanding of the restaurant situation in the United States, especially New York, and hope to get some insights during the data processing. While our platform mainly displays geographic data, it also combines some charts to provide users with more detailed information. Here are what you can get from our platform. 1. Geographical distribution of yelp restaurants in USA filtered by different categories. 2. Covid information of yelp restaurants. 3. Ratings distribution of yelp restaurants in USA. "],["data-sources.html", "Chapter 2 Data sources 2.1 Discription: 2.2 Basic Information: 2.3 Problems:", " Chapter 2 Data sources All four members in our team participated in the data collection process. We choose the data from Kaggle: https://www.kaggle.com/yelp-dataset/yelp-dataset And this is our original choice: https://www.yelp.com/dataset (The data We first downloaded from the yelp official website was due on Sep 2019, but another latest official dataset updated on kaggle was due on Feb 2020. After comparing these two datasets, we found that their sizes are the same, and columns are the same, so we thought the data from Kaggle was credible.) 2.1 Discription: We mainly looked into open data sources that can be found online or in the R packages instead of collecting data manually by ourselves, because we hope to work on large datasets with many variables and oberservations which will enable us to gain more interesting insignts. We explored datasets in R packages, and online resources such as Kaggle, NYC Open Data and some internet companies official data websites. Since those in R are relatively outdated and those on Kaggle do not have a reliable data source, we mainly focused on more recently published data, especially those released officially by internet companies. We finally chose a set of datasets released on Yelp Open Dataset because the sources of the data are authoritative; the data contain many variables that can be explored; and they are in a relatively standard format. 2.2 Basic Information: The set of datasets mainly contains three groups of data: information of businesses during COVID-19 period, demographic informations of businesses and other records such as reviews and tips. We can merge these datasets based on our interests. Each dataset has millions of obervations and contains both categorical variables and numerical variables. For example, the Yelp_covid dataset has more than 2,000,000 obervations and 9 variables. It provides information such as whether a business is temporarily closed or if it offers delivery or takeout services. The Yelp_business dataset has more than 2,000,000 obervations and 14 variables. It contains demographic variables such as locations and name as well as numerical variables such as bike parking spaces. The Yelp_tip dataset has 1,300,000 observations and 5 variables. It provides information such as user id, tip data and compliment numbers. 2.3 Problems: The most salient problem of the dataset we have noticed at this point is that all datasets are too large. It will take R very long time to work on them so we may need to figure out a way to subset them without loosing too many information. The other problem is that we wanted to look into changed on businesses before and after nCOVID-19. But since the Yelp_covid is a separate data and its promary key (business id) does not fully match with the other datasets, we may loose a lot of records while merging them together. So we may need to work on some more general aspects such as the over all proportions instead of summarizing by each business record. Additionally, some variable in the datasets need to be standardized. For example, the ‘text’ variable in the Yelp_tip dataset consists of comment sentences. We may need to think of a way to categorized them into positive or negative reviews. Finally, there are many missing variables in the dataset. Some columns have too many missing values that the entire column need to be deleted. "],["data-transformation.html", "Chapter 3 Data transformation 3.1 read data 3.2 change formats 3.3 Deal with categories 3.4 Deal with attributes 3.5 Output cleaned data", " Chapter 3 Data transformation 3.1 read data yelp_business_data &lt;- jsonlite::stream_in(file(&quot;data/yelp_academic_dataset_business.json&quot;)) 3.2 change formats Extracted the time table and put it back to the original dataset. Transformed all the character columns into factors except categories column. Extracted the attributes table as a new dataset, and deleted the attributes table and opening hours table. yelp_business_data$Monday &lt;- yelp_business_data$hours$Monday... yelp_business_data$city &lt;- factor(yelp_business_data$city)... yelp_business_attribute &lt;- yelp_business_data[,12] yelp_business_data &lt;- yelp_business_data[,c(-12,-14)] Extracted time information from character. Changed the opening hours of one column to two separate columns in order to capture the data more easily. yelp_business_data$monday1 &lt;- str_extract(yelp_business_data$Monday,&#39;^[0-9]+:[0-9]+&#39;) yelp_business_data$monday2 &lt;- str_extract(yelp_business_data$Monday,&#39;[0-9]+:[0-9]+$&#39;)... yelp_business_data &lt;- yelp_business_data[,-c(13:19)] Transformed the character into date type. yelp_business_data$monday1 &lt;- strptime(yelp_business_data$monday1,&#39;%H:%M&#39;)... yelp_business_data$categories &lt;- tolower(yelp_business_data$categories) 3.3 Deal with categories Filtered the dataset by applying if there is a ‘restaurant’ in the categories. Deleted some rows with ‘restaurant’, but not what we want. cate &lt;- strsplit(yelp_business_data$categories,&#39;,&#39;) match = &#39;restaurants&#39; boolean1 &lt;- grepl(match,cate) yelp_restaurant &lt;- yelp_business_data[boolean1,] cate &lt;- strsplit(yelp_restaurant$categories,&#39;,&#39;) match2 = &#39;shopping&#39; boolean2 &lt;- grepl(match2,cate) yelp_restaurant &lt;- yelp_restaurant[!boolean2,] yelp_restaurant &lt;- subset(yelp_restaurant,!is.na(yelp_restaurant$business_id)) yelp_restaurant$categories &lt;- gsub(&#39;restaurants[,]*&#39;,&#39;&#39;,yelp_restaurant$categories) yelp_restaurant$categories &lt;- gsub(&#39;food[,]*&#39;,&#39;&#39;,yelp_restaurant$categories) Extracted words from categories. cate &lt;- strsplit(yelp_restaurant$categories,&#39;,|&amp;|[)]|[(]|[ ]|/&#39;) yelp_restaurant1$categories &lt;- substr(cate,2,1000) Word cloud of cleaned categories. text&lt;-yelp_restaurant1$categories mixseg&lt;-worker(&quot;mix&quot;) a&lt;-segment(text,mixseg) stopwords &lt;- read.table(&quot;stopwords.txt&quot;) stopwords &lt;- as.vector(stopwords[,1]) wordResult &lt;- removeWords(a,stopwords) freq&lt;-table(wordResult) wordcloud2(freq,size=5) Deeper cleaning and merging of categories. yelp_restaurant1$categories &lt;- gsub(&#39;&quot;veggies&quot;|&quot;vegetarian&quot;&#39;,&#39;&quot;vegan&quot;&#39;,yelp_restaurant1$categories)... yelp_restaurant1$categories &lt;- gsub(&#39;[(]|[&quot;]|[)]|[,]&#39;,&#39;&#39;,yelp_restaurant1$categories) Extracted unique word for each row. cate &lt;- strsplit(yelp_restaurant1$categories,&#39; &#39;) for (i in 1:nrow(yelp_restaurant1)) {yelp_restaurant1$categories[i] &lt;- str_c(unique(cate[[i]]),collapse=&#39; &#39;)} Word cloud about cuisine. text1&lt;-yelp_restaurant1$categories mixseg1&lt;-worker(&quot;mix&quot;) b&lt;-segment(text1,mixseg1) wordResult1 &lt;- removeWords(b,stopwords) new_list &lt;- c(&#39;acai&#39;,&#39;alcohol&#39;,&#39;barbeque&#39;,&#39;breakfast&#39;,&#39;brunch&#39;,&#39;buffets&#39;,&#39;delivery&#39;,&#39;desserts&#39;,&#39;dinner&#39;,&#39;drinks&#39;,&#39;flourfood&#39;,&#39;fruits&#39;,&#39;imported&#39;,&#39;local&#39;,&#39;organic&#39;,&#39;salad&#39;,&#39;vegan&#39;,&#39;meat&#39;,&#39;kosher&#39;) wordDraw &lt;- removeWords(wordResult1,new_list) freq1&lt;-table(wordDraw) wordcloud2(freq1,size=12) Cuisine countries. ## wordDraw ## afghan african american arabian argentine armenian ## 155805 144 157 11390 73 56 11 ## asian australian austrian bangladeshi basque belgian brazilian ## 2215 11 8 27 30 52 97 ## british burmese cajun cambodian canadian caribbean chinese ## 155 13 315 46 1915 689 4961 ## creole cuban czech dominican egyptian ethiopian filipino ## 315 85 6 17 21 87 261 ## french german greek halal hawaiian hungarian indian ## 1274 149 1132 721 361 33 1639 ## indonesian iranian irish italian japanese korean latin ## 30 205 217 4918 3828 1021 646 ## lebanese malaysian mediterranean mexican middleeastern mongolian moroccan ## 284 78 1984 5166 1433 52 76 ## nepalese othercountries pakistani persian puerto russian salvadoran ## 73 49 512 205 33 58 81 ## scandinavian singaporean spanish sri syrian thai turkish ## 12 32 995 47 18 1541 154 ## ukrainian venezuelan vietnamese ## 26 41 1396 Delete rows without these categories. wordResult1 = wordResult1[wordResult1 != &#39;&#39;] exist_cate &lt;- str_c(wordResult1,collapse=&#39;|&#39;) boolean_cate &lt;- str_detect(yelp_restaurant1$categories,exist_cate) yelp_restaurant1 &lt;- yelp_restaurant1[boolean_cate,] Added a new column for cuisine countries. k &lt;- names(freq1) country &lt;- str_c(k[-1],collapse=&#39;|&#39;) yelp_restaurant1$country &lt;- str_match_all(yelp_restaurant1$categories,country) yelp_restaurant1$country &lt;- gsub(&#39;c[(]+|[)]+|[&quot;]&#39;,&#39;&#39;,yelp_restaurant1$country) yelp_restaurant1$country &lt;- gsub(&#39;[&quot;, &quot;]&#39;,&#39; &#39;,yelp_restaurant1$country) yelp_restaurant1$country[yelp_restaurant1$country == &#39;character(0&#39;] = NA Added new columns for categories. yelp_restaurant1$acai &lt;- ifelse(str_detect(yelp_restaurant1$categories,&quot;acai&quot;),1,0)... yelp_restaurant1 &lt;- yelp_restaurant1[,-grep(&#39;categories&#39;,colnames(yelp_restaurant1))] 3.4 Deal with attributes yelp_restaurant1$credit &lt;- str_detect(yelp_restaurant1$credit,&quot;True&quot;) yelp_restaurant1$attire &lt;- gsub(&quot;u[&#39;]+|[&#39;]&quot;,&#39;&#39;,yelp_restaurant1$attire)... 3.5 Output cleaned data write.csv(x = yelp_restaurant1,file = &quot;yelp_cleaned_data.csv&quot;) "],["missing-values.html", "Chapter 4 Missing values", " Chapter 4 Missing values The data had null values. To preserve all the information, we imputed or dropped the rows and columns containing null values while conducting exploratory analysis that made use of these features. We construct a visna analysis to analyze the missing values pattern for the variables that we would be using in our exploratory analysis. From this graph, we see that the pattern of missing values are very diverse and there are just a couple of patterns that have a high frequency. However, the characteristics of these patterns seem to be very complex and random. What we do see clearly is that there are a few varables with high proportion of missing entries. The most prominent ones are appointment, credit and bikepark. We may need to try to avoid using these variables. To analyze the number of missing values in each variable, we counted the number of NAs in each column and plot them in histogram: We found that there are plenty of missing data in about 20 columns to the right, so later we just delete all these columns. For the columns in the middle, deleting or not depends on which part of data do we need, if we want to analysis some specific region, then situation may change. For columns with 50000-100000 missing data, if these columns are important to our analysis, we may just delete the missing data, if not, delete the whole column. "],["results.html", "Chapter 5 Results 5.1 Spatial Data Analysis 5.2 Time Analysis 5.3 Yelp Rating Analysis 5.4 Restaurant Services and Attribute Analysis 5.5 Covid Impact Analysis", " Chapter 5 Results In this section, we will detail our analysis to the questions of interest mentioned in the introduction and gain preliminary insights through exploratory data analysis and visualization. We have divided it into four subsections that aim to answer the questions through a variety of different visualization. Spatial Data Analysis Demand and Price Analysis User Review (Textual Data) Mining Other Interesting Insights 5.1 Spatial Data Analysis In this part, we will explore some basic variables from our dataset using spatial visualizations and will answer questions relating to density of restaurants, variations in opening or not, and cuisine. We will do all these detailed analysis based on a subset of data in Arizona. 5.1.1 Whole Dataset Before we are going to show you some interesting findings, let’s have a look at our whole data. yelp_grouped &lt;- yelp_restaurant_data %&gt;% group_by(state) %&gt;% summarise(long=mean(longitude),lat=mean(latitude),n=n()) leaflet(data = yelp_grouped)%&gt;% setView( lat=37, lng=-97 , zoom=4)%&gt;% addTiles() %&gt;% addCircleMarkers(~long, ~lat,label=~state,radius = ~n/600,stroke=FALSE,fillOpacity=0.5, popup = paste(&quot;State: &quot;, yelp_grouped$state, As we can see, Airbnb only provides data of several cities like Montreal and Waterloo in Canada, Pittsburgh, Charlotte, Urbana-Champaign, Phoenix, Las Vegas, Madison, Cleveland in U.S., so we will focus on a single area to do our analysis, and we decide to use Arizona which includes city Phoenix with biggest dataset. 5.1.2 Arizona Dataset library(rgdal) # group the data by zipcode: yelp_AZ &lt;- yelp_restaurant_data[yelp_restaurant_data$state==&#39;AZ&#39;,] yelp_AZ_postal&lt;- yelp_AZ %&gt;% group_by(postal_code) %&gt;% summarise(n=n()) yelp_AZ_postal &lt;- yelp_AZ_postal[c(-1,-2),] names(yelp_AZ_postal) &lt;- c(&#39;GEOID10&#39;,&#39;n&#39;) # Read shapefile: my_zip &lt;- readOGR( dsn= path.expand(&quot;Data/zip_shape&quot;) , layer=&quot;cb_2018_us_zcta510_500k&quot;, verbose=FALSE ) # Create a color palett for the map: my_zip@data &lt;- left_join(my_zip@data,yelp_AZ_postal,by=&#39;GEOID10&#39;) mypalette &lt;- colorNumeric( palette=&quot;viridis&quot;, domain=my_zip@data$n, na.color=&quot;transparent&quot;) mypalette(c(45,43)) # choropleth map: m &lt;- leaflet(my_zip) %&gt;% addTiles() %&gt;% setView( lat=33.6, lng=-112 , zoom=9) %&gt;% addPolygons( fillColor = ~mypalette(n), stroke=FALSE, fillOpacity=0.5) %&gt;% addLegend(&quot;topright&quot;, pal = mypalette, values = my_zip@data$n, title = &quot;quantity&quot;) m Choropleth: Density of restaurants in Arizona. This is a plot with all the restaurants in Arizona grouped in zipcode clusters. You can further click on each restaurant to see details like Name, address, stars and is open or not. This visualization helps us to basicly understand how restaurants are distributed across zipcode zones. We can see from the map that maximum restaurants are clustered around Tempo, and Scottsdale, followed by some areas around Phoenix, and the area with fewer restaurants are a little bit far from Phoenix city. Now, because the epidemic is very serious, we really want to know if restaurants are still open. Here, we work with the is_open data within the dataset to see if there is any pattern about opening or not. In this interactive plot, you can click on the circles to see their basic informatin. AZ_palette &lt;- colorFactor(palette=&#39;RdYlBu&#39;, domain=yelp_AZ$is_open, levels = NULL, ordered = FALSE, na.color = &quot;#808080&quot;, alpha = FALSE, reverse = FALSE) leaflet(data = yelp_AZ)%&gt;% addTiles() %&gt;% addCircleMarkers(~longitude, ~latitude,label=~name,radius = 5,stroke=FALSE, fillColor = ~AZ_palette(is_open), popup = paste(&quot;Name: &quot;,yelp_AZ$name, &quot;&lt;br /&gt; Address:&quot; ,yelp_AZ$address, &#39;&lt;br /&gt; Stars: &#39;,yelp_AZ$stars, &#39;&lt;br /&gt; Open: &#39;,yelp_AZ$is_open)) %&gt;% addLegend(&quot;topright&quot;, pal = AZ_palette, values = yelp_AZ$is_open, title = &quot;Is open&quot;) From the plot, it is easy find differences between open restaurants and closed restaurants. For example, there are more open restaurants than closed restaurants, and they are more widely distributed. Also, closed restaurants are mostly concentrated in the city or town centers. This makes sense, because these areas are more dangerous than other areas, thus people may tend to suspend business to protect themselves. And since there are more restaurants located there, the probability of being closed must be lager than in other areas. It is interesting to observe these patterns, and if you are living in Phoenix, you may have some deeper and personal findings from this graph. What we are also interested in is how the ratings are related with geographic data. We all know that people can rate restaurants on Yelp, and if we want to find somewhere to eat, we must notice that score. The stars more or less influenced our decisions. star_palette &lt;- colorNumeric(palette=&quot;plasma&quot;, domain=yelp_AZ$stars, na.color=&quot;transparent&quot;) leaflet(data = yelp_AZ)%&gt;% addTiles() %&gt;% addCircleMarkers(~longitude, ~latitude,label=~name,radius = 5,stroke=FALSE,fillColor = ~star_palette(stars), popup = paste(&quot;Name: &quot;,yelp_AZ$name, &quot;&lt;br /&gt; Address:&quot; ,yelp_AZ$address, &#39;&lt;br /&gt; Stars: &#39;,yelp_AZ$stars, &#39;&lt;br /&gt; Open: &#39;,yelp_AZ$is_open)) %&gt;% addLegend(&quot;topright&quot;, pal = star_palette, values = yelp_AZ$stars, title = &quot;Stars&quot;) The graph shows that the centre of Phoenix, a small area in Tempo, Scottsdale, and the upright area in AZ have more stars. Glendale, and unexpectly, area between Phoenix and Tempo have least stars. Interestingly, if we zoom in the map, we will find that the restaurants with very few stars are all in an airport called ‘Phoenix Sky Harbor International Airport’. (And now it seems make sense :) After we have seen the overall distribution, we may have question: Is the star distriburion related to the cuisine? We choose the top3 amount cuisines and let’s have a look. bool_american &lt;- str_detect(yelp_AZ$country,&#39;american&#39;) yelp_AZ_american &lt;- yelp_AZ[bool_american,] yelp_AZ_american &lt;- subset(yelp_AZ_american,!is.na(yelp_AZ_american$business_id)) leaflet(data = yelp_AZ_american)%&gt;% addTiles() %&gt;% addCircleMarkers(~longitude, ~latitude,label=~name,radius = 5,stroke=FALSE,fillColor = ~star_palette(stars), popup = paste(&quot;Name: &quot;,yelp_AZ_american$name, &quot;&lt;br /&gt; Address:&quot; ,yelp_AZ_american$address, &#39;&lt;br /&gt; Stars: &#39;,yelp_AZ_american$stars, &#39;&lt;br /&gt; Open: &#39;,yelp_AZ_american$is_open)) %&gt;% addLegend(&quot;topright&quot;, pal = star_palette, values = yelp_AZ_american$stars, title = &quot;Stars&quot;) We found that different cuisines actually have similar score distributions. The star may be more related to location than cuisine. Let us look at the overall distribution of cuisine in AZ. Here we choose the top9 amount cuisines. This plot shows clearly that there are some clusters of mexican restaurants, american restaurants are located everywhere, chinese restaurants have a trend to the downright, while indian are mostly located on the top and downright. 5.2 Time Analysis The following plot shows how opening hours are different from different days in a week. 5.3 Yelp Rating Analysis 5.3.0.1 Overview of Stars Rating graphical distribution In This section, we will mainly focusing on analyzing the distribution and characteristics of stars rating in Yelp data. The aim of this part is to answer several question: Is the rating distribution has geographical features? What is the distribution of Ratings among different Restaurant type? What is the distribution of Ratings among different country style? And what is the relationship between stars rating and the price level of this restaurant？ First of all, we want to look deep into the Geographical distribution characteristics for yelp stars rating data, to study the relationship between state distribution and the restaurants’ stars rating. micromapST(d, panelDesc01, sortVar=13,ascend = FALSE, title=c(&quot;States Average Stars Rating in 2020&quot;, &quot;Map distribution &amp; cleverdot plot&quot;) ) In this section, we use micromapST packages to analyze the yelp stars rating dataset. As for this packages, we can have a clear and straightforward idea about the state geographical spot and how it related to the attribute we are looking for. Due to the data size limitation, we have to consider the influence from too small data size for some states which can dramatically distort the relationship and results. Accordingly, we choose stars ratings, state, and review count as data input. In each of the above figures, we can see three columns: the left one is the the map of state in US which colored state means that this state is chosen, the second column is cleverdot plot for average stars ratings in each state, and the last column is about the total number of review counts in each state. The first figure is sorted by ratings, and the second one is sorted by counts. In the first picture, we can see that the average ratings of each state ranging from 1-5, mainly concentrated around 3.5 and it seems that there is some relation among states distribution. However, we have to take number of counts into consideration, so we switch the sorting method from ratings to ratings to total number. In the second picture, we can see that for total number of counts large enough stats (which can draw a more stable and more convicing idea), the stars rating is relatively constant around 3.5 with no obvious geographical features. So, we can come to the conclusion that the stars ratings in Yelp do not present obvious geographical features among states. And for deeper analysis in stars ratings, we choose sub-dataset in Arizona state as a representative, which has top2 capacity of total review counts. 5.3.0.2 Stars Rating Analysis in Arizona yelp_az &lt;- yelp_clean_data[yelp_clean_data$state == &quot;AZ&quot;,] ggplot(yelp_az, aes(x=stars)) + geom_bar() In This section, we will mainly focus on the sub-dataset in Arizona State to represent the total ratings, as we conclude that there is limited difference in stars ratings among different states. In the fist place, we draw a overview for stars distribution in Arizona State, we use ggplotbar chart to display the distribution of this uncontinuous variable of stars rating, and we can see a left-skewed normal distribution and the dashed blue line is the mean for ratings. In the following part, we will look into the relationship of between ratings and Restaurant type and Country style. 5.3.0.2.1 1. The distribution of Ratings among different Restaurant category ggplot(yelp_az_tidy,aes(stars))+geom_bar()+facet_wrap(~category) ggboxplot(yelp_az_tidy) In this section, Let’s take a close look at the relationship between The distribution of Ratings and different Restaurant category. We use ggplotbar chart and ggboxplotas our visualization tools. The Restaurant category means the main business of this restaurant, which includes alcohol/brunch/drinks/meat/vegan etc.. The question is whether the rating distribution is constant or will be affected by different main business. In the first chart, we can see rating distribution bar chart grouped by different Restaurant category. The different average height in different plot is driven by the size of data. We can see that almost every Restaurant category shows a left-skewed normal distribution in ratings, which means people are more tending to give lower rating that higher rating centered around 3.5-4, and this is constant with the total distribution. In the second chart, we can see that rating distribution boxplot grouped by different Restaurant category. In this figure, we can see more quantitative for the difference among categories. It showes that dessert/drinks/organics often has higher average ratings compared to others, specially for organics(But organics has relatively low datasize and this will challenge the calidity of the results). And for meat, although has almost same average ratings with others, it has wider range of rating distribution and less-likely normal distribution, which can indicate that people may perform more diversified opions toword meat restaurant. 5.3.0.2.2 2. The distribution of Ratings among different Countries country_data$american &lt;- ifelse(str_detect(country_data$country,&quot;american&quot;),1,0) country_tidy &lt;- gather(data = country_data, key = &quot;Country&quot;, value = &quot;value&quot;, 4:16) ggplot(yelp_az_tidy,aes(stars))+geom_bar()+facet_wrap(~category) ggboxplot(yelp_az_tidy) In this section, There is another special perspective we’d like to explore while analyzing the relationship between stars ratings and country styles. We use ggplotbar chart and ggboxplotas our visualization tools as well. The country style means the food style origin of these restaurant, which includes American/Mexican/Chinese/Japanese etc.(We choose top16 country for analysis). The question is whether the rating distribution is constant or will be affected by different country style. In the first chart, we can see rating distribution bar chart grouped by different country style. The different average height in different plot is driven by the size of data. We can see that almost every country style shows a left-skewed normal distribution in ratings. And we can also get the information that American and Mexican food has a predominated role in US compared to others. In the second chart, we can see that rating distribution boxplot grouped by different country style. In this figure, we can see more quantitative for the difference among countries. It shows that the stars rating distribution is relatively stable and constant among different country styles, and we can divide it into two parts: one part for first 4 country style (American/Mexican/Chinese/Italian), and another group for other country styles. The first groups has relatively higher datasize, lower average ratings and wider rating distribution. The second group often shows higher average ratings, narrower rating distribution and also lower data size. 5.3.0.2.3 3. Stars Rating VS Price level count_data &lt;- price_data %&gt;% count(price,stars) ggplot(count_data, aes(price, stars, fill= n)) +geom_tile() In this section, Let’s take a close look at the relationship between Stars Ratings and Restaurant price level. In our dataset, we have price level and ratings for each restaurant, so we decide to use geom_tile()heatmap to visualize this relationship. The price level is ranging from 1-4($,$$,$$$,$$$$), and the stars is ranging from 1-5 (minimum interval of 0.5). As shown in the figure, for low price level (1/2, represents $/$$in yelp website), stars rating are much more intended to be normal-distributed with higher tendency for moderate ratings not extremely high praise or low ratings; However, for low price level (3/4), the data size is decreasing and also, the distribution of ratings are more like constant not normal, which may represents that not higher price equal to higher rating and more pleasant enjoyment. 5.4 Restaurant Services and Attribute Analysis In this section, we will analyze some attributes and services provided by the restaurants, in order to illustrate whether a certain features will affect a restaurant’s rating or pricing. Particularly, we are curious about how much the supplementary services besides food itself account for the the difference between high-rated and low-rated, expensive and economical places. Firstly, in order to study the relationship between supplementary services and the restaurants’ pricing, let’s look at an overview grouped bar chart. The x axis is four value-added services that we will pay attention to in the following analysis: Parking (whether there is convenient parking area near the restaurant), Outdoor Seats (whether the restaurant provides seats for outdoor waiting), WiFi (whether there provides WiFi service for clients), TV (whether the restaurant is equipped with one or more TV for entertainment). The y axis shows the number counts. And the two colors stand for “yes or no” conditions. From the graph, we could know that generally, the most of restaurants are pricing at $$ level and at the $$$ or higher level, there are relatively few restaurants. As for the services, it’s common for the four pricing levels that the number of restaurants which provide parking area is higher than the one that not, which is in the same condition as the WiFi service. However, for the outdoor seats, the high pricing level restaurants tend to set up, while the majority of economical level places don’t. Let’s take a close look at the relationship between the pricing and the supplementary services. By plotting the alluvial graph, we’d like to address whether the higher prices are set, the more value-added services are provided. The most obvious finding is that higher proportion of $$$ and $$$$ level restaurants offer convenient parking area or parking services than the other two levels. Considering the reasons, this may be related to the locations of higher pricing level restaurants. Because of the expensive prices, they either have more budgets to locate at a convenient place, or have to do so in order to match up to their overall standing. As for outdoor seats, generally a half of all restaurants provide while half not, and there is not a significant pattern among different pricing levels. It’s easy to understand because not every restaurant needs to enter after a long time of waiting. The same data result happens to WiFi, too. It seems quite confusing to us at the beginning as we thought that the majority of restaurants provide WiFi services nowadays, based our intuition and experience. However, it also makes sense that restaurants vacillate between providing a nice amenity and dragging clients’ attention from the virtual world back to their food. However, the situation of TV equipment is totally different. Most of restaurants have one or more TVs in their store, which a scientific findings may help to explain: people tends to eat more while watching a video or shows. There is another special perspective we’d like to explore while analyzing the relationship between price and services. In our mind, we meet more “rules”, like dressing code, and behave more conservatively at some expensive restaurants. Is this true, or just stereotypes? The alluvial plot mainly focus on three attributes: Attire (whether the restaurant requires a dress code), AppointmentOnly (whether customers can access the restaurant by appointment only or not), Noise (how’s the noise level inside the restaurant). From the alluvial plot, we can see that although higher proportion of pricey restaurants tend to set a “dressy” attire requirement, not every $$$ and $$$$ restaurants have a dress code. Mostly, $$ and $ price level restaurants allow casual attire, while a few of them require dressy code. As for appointment, very few of restaurants are accessed only by appointment, which are mainly from $$ and $$$ price level. Sometimes, restaurants with distinguishing membership or extreme popularity may need appointment only regulations to manage their customer volume and essential ingredient purchase. When it comes to noise, this is a joint effect of environment and customers’ behavior. Most of the pricey restaurants have average noise, while the economical restaurants are more inclined to go extreme, either quiet or loud. Providing a relatively quiet environment and also making customers feel comfortable to chat and chill while eating is an art of running a perfect restaurant. As we conclude above, various pricing level restaurants have some differences in supplementary services providing and consumer regulations. Now, let’s further question whether thess differences matter in the restaurant ratings. On consideration of the distribution, we divided the ratings into 5 level groups. As the alluvial plot shows, the different color flows go roughly even into each categories, which demonstrates that there is not a strong relationship between restaurant ratings and value-added services. On one hand, technology development helps restaurants to learn from each other faster and renders them easy to update these hardware. Thus, when the supplementary services become common, it’s hard to distinguish from others by providing the same thing. On the other hand, value-added services include but are not limited to these four factors. Besides the basic amenities, customers may care more about some true value-added and distinctive features, like stylish decorations, meticulous waiters. 5.5 Covid Impact Analysis In this section, we analyze how restaurants were affected by nCovid-19. The covid dataset from Yelp includes how restaurants respond to the pandemic and the main purpose of this section is to explore how the demographic and behavioral data of restaurats are correlated with the Covid effects. data[&quot;Open after covid&quot;] &lt;- (as.logical(data$`delivery or takeout`) | as.logical(data$`Grubhub enabled`)) &amp; data$is_open Since the original datasets give no information on the Opening status of restaurants during the pandemic, I created a new column “Open after covid” indicating whether restaurants are still open inferring from other variables. In particular, there are three conditions I looked into: whether the restaurant is still open before the pandemic, whether the restaurants offer delievery service, and whether the restaurant collarborate with Doordash. The process is printed as above. With the information about the opening status of restaurants, I am interested in how it is distributed over different states. Bellow is a Cleveland dot plot of opening status of restaurants over States faceted by delivery availability. We can clearly see that the opened restaurants during Covid has much higher frequency in delivery services and those closed during Covid has higher frequency in non-delivery. Moreover, if we just look at the opened restaurants panel, we can see that the number of non-delivery restaurants are very low (very close to 0) and only exists in the states where more data are collected. On the other hand, if we look at the closed restaurants panel, the differences between delivery available versus not avalaible restayrants do not differ much in the states where more data are collected. To further analyze the problem, bellow is a Parallel Coordinate Plots where the x-axis is a series of variable related to Covid impacts and restaurants attributes. I colored them using opening status. From the graph, we can see that the relationship between the variables are relatively random: there is no strong evidence of either postive relationships or negative relationships. However, by coloring the data using opening status, we can see that most non-open restaurants do not have specific policies against Covid impacts and they do not provide delivery options. The rating (star) of the restaurants seems to be not correlated with opening status according the plot as the non-open restaurants have similar number of stored distributed over each star level. As for the review count, since the data is very imbalances: having large number of low review counts, we cannot infer its relationship with other variables. Lastly, we are interested in the difference of Covid impacts over different States. Bellow is the Statebin plot of open store proportions. As mentioned above, we have a strong imbalance in our data. Some States only have a few data points while some States have a large number of data collected. Trying to alleviate this problem, instead of ploting the frequencies of opening restaurants in each States, we decided to compute the proportion of open restaurants (number of open restaurants over number of total restaurants recorded in the data in each state). Since not all States are included in our data, we only plotted the States that exists in the dataset. We did not fill in the rest of the States because by marking them with a color or NA can be misleading while comparing differences between States. From the Statebin plot, we see that NY has a relatively high proportion of open restaurants followed by TX, OH and NC. There are several States with 100% proportions and these are likely outliers: 100% occurs mostly because there are only a few data collected and these collected restaurants happened to be opening. "],["interactive-component.html", "Chapter 6 Interactive component 6.1 Shiny App: Word Cloud for Yelp Key Words 6.2 Shiny App: Change the Sorting Method for Rating Distribution", " Chapter 6 Interactive component 6.1 Shiny App: Word Cloud for Yelp Key Words We initially deployed our interactive plots to https://edavyelpanalysis.shinyapps.io/edav_wordcloud/ but later encountered technical issues with the server. So we aslo screen shot our images in the following. This Shiny App serves as an extension to our previous Word-Vector analysis of the restaurant category and country style. Once the word vector has been built, anyone can adjust the input limitation (like: minimum frequency and max number of words) and build a word-cloud. User can slide the bar and set the “max words” and “min frequency” in the word cloud. A valid query word would be one that is present in the corpus, else it wouldn’t be part of the word vector. It is easy to think of such words; It help the user to interact with the word cloud generation. 6.2 Shiny App: Change the Sorting Method for Rating Distribution Screenshot for Interactive Shiny APP:Rating Distribution . In addition to directly using pdf or pictures to display Ratings distribution under different sorting methods, we can use shiny app to adopt a interactive way of switching the sorting method. By clicking the radio button, we can alter the choice of sorting method, and compare the result much more directly. In main panel, we inserted the micromap result as images that is linked to different choices. "],["conclusion.html", "Chapter 7 Conclusion", " Chapter 7 Conclusion As we discussed in previous sections, there are many limitations in our analysis due to problems within the dataset and technical issues. The data we obtained is heavily imbalanced: it focuses on collectinhg large number of data from a few States. Moreover, many variables have too many missing values, some of which are essential to our analysis. We had to omit them all in the end and unable to draw inderences from them. Lastly, we encountered technical issues when deploying our interactive image from Shiny. We only got the static images in the end. Followings are our future directions: We may want to fix the imbalance issues of the data over States in the future. We may either analyze the States with large number of observations or try to find more data from other sources on other States to compare restaurants characteristics over different States. We found that restaurant opening and closing time differ over locations and types of food they serve. We may want to create an interactive plot using D3 or Shiny to show this relationship. Since the nCovid-19 is still an ongoing pandemic, we are interested on future data from Yelp when the pandemic is over. We are interested in: whether the restaurants closed during the pandemic, ratings on delivery APPs during the pandemic, etc. "]]
